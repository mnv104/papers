@comment{{This file has been generated by bib2bib 1.93}}

@comment{{Command line: /usr/bin/bib2bib -c 'keywords:"dedup" or $type="PROCEEDINGS" or $key:"fagin79.*"' /tmp/readings.bib}}


























@article{fagin79extendiblehashing,
  author = {Ronald Fagin and Jurg Nievergelt and Nicholas
                  Pippenger and H. Raymond Strong},
  title = {Extendible Hashing---A Fast Access Method For
                  Dynamic Files},
  journal = {ACM Transactions on Database Systems},
  month = sep,
  year = 1979,
  volume = 4,
  number = 3,
xpages = {315--344},
  url = {http://doi.acm.org/10.1145/320083.320092},
  abstract = {Extendible hashing is a new access technique, in
                  which the user is guaranteed no more than two page
                  faults to locate the data associated with a given
                  unique identifier, or key. Unlike conventional
                  hashing, extendible hashing has a dynamic structure
                  that grows and shrinks gracefully as the database
                  grows and shrinks. This approach simultaneously
                  solves the problem of making hash tables that are
                  extendible and of making radix search trees that are
                  balanced. We study, by analysis and simulation, the
                  performance of extendible hashing. The results
                  indicate that extendible hashing provides an
                  attractive alternative to other access methods, such
                  as balanced trees.}
}

@inproceedings{hong04sandedup,
  author = {Bo Hong and Demyn Plantenberg and Darrell D. E. Long
                  and Miriam Sivan-Zimet},
  title = {Duplicate Data Elimination in a {SAN} File System},
  crossref = {mss04},
xpages = {301--314},
  url = {http://www.ssrc.ucsc.edu/~darrell/Papers/MSST-Hong-04.pdf},
  keywords = {dedup},
  abstract = {Duplicate Data Elimination (DDE) is our method for
                  identifying and coalescing identical data blocks in
                  Storage Tank, a SAN file system. On-line file
                  systems pose a unique set of performance and
                  implementation challenges for this feature. Existing
                  techniques, which are used to improve both storage
                  and network utilization, do not satisfy these
                  constraints. Our design employs a combination of
                  content hashing, copy-on-write, and lazy updates to
                  achieve its functional and performance goals. DDE
                  executes primarily as a background process. The
                  design also builds on Storage Tank's FlashCopy
                  function to ease implementation.  \par We include an
                  analysis of selected real-world data sets that is
                  aimed at demonstrating the space-saving potential of
                  coalescing duplicate data. Our results show that DDE
                  can reduce storage consumption by up to 80\% in some
                  application environments. The analysis explores
                  several additional features, such as the impact of
                  varying file block size and the contribution of
                  whole file duplication to the net savings.},
  annote = {
    \par Not as interesting as I had hoped. This is based on IBM's
    Storage Tank, which uses direct SAN access for file data I/O, but
    still has a centralized server for metadata management. The
    deduplication in this paper is mostly the responsibility of the
    server.

    \par They use a write log that records hashes and is processed
    lazily to discover duplicates and rely on filesystem-level block
    redirection with COW in order to perform coalescing.

    \par They avoid inconsistency between their fingerprint database
    and block contents by using a really strict form of copy-on-write
    where the first write to any block by a client following the
    acquisition of the file lock causes the client to copy that
    block. They claim there's no impact of this, but make no mention
    of locality.

    \par The server changes block mappings without notifying clients
    by depending on this copy-on-write behavior and by not reclaiming
    blocks that may be in client's block map caches. Apparently when
    the file system is under light load they just revoke all of the
    locks and clean things up.

    \par They perform heat optimization by maintaining log epochs and
    only deduplicating blocks that appear in the second-to-last epoch
    but not in the last epoch. No data is provided on the
    effectiveness of this.
  }
}

@inproceedings{douceur02farsitededup,
  author = {J.R. Douceur and A. Adya and W.J. Bolosky and
                  P. Simon and M. Theimer},
  title = {Reclaiming Space from Duplicate Files in a
                  Serverless Distributed File System},
  crossref = {icdcs02},
xpages = {617--624},
  url = {http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1022312},
  keywords = {dedup},
  abstract = {\par The Farsite distributed file system provides
                  availability by replicating each file onto multiple
                  desktop computers. Since this replication consumes
                  significant storage space, it is important to
                  reclaim used space where possible. Measurement of
                  over 500 desktop file systems shows that nearly half
                  of all consumed space is occupied by duplicate
                  files. We present a mechanism to reclaim space from
                  this incidental duplication to make it available for
                  controlled file replication. Our mechanism includes:
                  (1) convergent encryption, which enables duplicate
                  files to be coalesced into the space of a single
                  file, even if the files are encrypted with different
                  users' keys; and (2) SALAD, a Self-Arranging Lossy
                  Associative Database for aggregating file content
                  and location information in a decentralized,
                  scalable, fault-tolerant manner. Large-scale
                  simulation experiments show that the duplicate-file
                  coalescing system is scalable, highly effective, and
                  fault-tolerant.},
  annote = {
    \par Applies deduplication to the decentralized Farsite file
    system.  ``Lossy'' - not all duplicates are detected, but most
    are.  Piggybacks on the Windows Single Instance Store to perform
    local deduplication.  Farsite is only responsible for getting the
    data to the right place.

    \par Deduplicates on the level of files, ''not'' blocks.  This is
    forced by the design of Farsite itself, which operates on files,
    not blocks.  There are no provisions in Farsite (according to the
    Farsite paper
    (\url{http://portal.acm.org/citation.cfm?id=844128.844130}) for
    cross-host writes and the like.

    \par Uses a custom, somewhat wonky hypercube DHT to move files to
    hosts based on their content hashes
    
    \par Interesting experimental results: Over 585 test machines,
    46\% of the 685 GB of file data could be removed by deduplication.
  }
}

@inproceedings{zhu08datadomain,
  author = {Benjamin Zhu and Kai Li and Hugo Patterson},
  title = {Avoiding the Disk Bottleneck in the {D}ata {D}omain
                  Deduplication File System},
  crossref = {fast08},
xpages = {269--282},
  url = {http://www.usenix.org/event/fast08/tech/zhu.html},
  keywords = {dedup},
  abstract = {Disk-based deduplication storage has emerged as the
                  new-generation storage system for enterprise data
                  protection to replace tape libraries. Deduplication
                  removes redundant data segments to compress data
                  into a highly compact form and makes it economical
                  to store backups on disk instead of tape. A crucial
                  requirement for enterprise data protection is high
                  throughput, typically over 100 MB/sec, which enables
                  backups to complete quickly. A significant challenge
                  is to identify and eliminate duplicate data segments
                  at this rate on a low-cost system that cannot afford
                  enough RAM to store an index of the stored segments
                  and may be forced to access an on-disk index for
                  every input segment.  \par This paper describes
                  three techniques employed in the production Data
                  Domain deduplication file system to relieve the disk
                  bottleneck. These techniques include: (1) the
                  Summary Vector, a compact in-memory data structure
                  for identifying new segments; (2) Stream-Informed
                  Segment Layout, a data layout method to improve
                  on-disk locality for sequentially accessed segments;
                  and (3) Locality Preserved Caching, which maintains
                  the locality of the fingerprints of duplicate
                  segments to achieve high cache hit ratios. Together,
                  they can remove 99\% of the disk accesses for
                  deduplication of real world workloads. These
                  techniques enable a modern two-socket dual-core
                  system to run at 90\% CPU utilization with only one
                  shelf of 15 disks and achieve 100 MB/sec for
                  single-stream throughput and 210 MB/sec for
                  multi-stream throughput.},
  annote = {
    This system is very similar to Venti.  It's a centralized archival
    system with no deletion and in-band deduplication.  At it's core,
    it's a standard CAS system in which blocks are referenced by
    content hash and stored in a global block pool.  The pool is
    divided into ``containers'', which are equivalent to Venti arenas.
    Containers store variable-sized segments.
    
    \par The key optimizations discussed are
    \begin{description}
    \item[Summary Vector] They keep a large in-memory Bloom filter of
      the hashes that are present in the index.  This allows writes to
      be short-circuited if the hash is already in the filter.
    
    \item[Stream-Informed Segment Layout] New segments are appended to
      the current container in the same order they appear in the input
      in order to maintain spatial locality.  This is kind of a
      no-brainer, but they have to go to extra trouble since there may
      be multiple unrelated backup streams coming in to the system at
      the same time.
    
    \item[Locality Preserved Caching] All of the metadata for the
      segments in a container is stored at the beginning of the
      container.  Thus, when it is necessary to read one segment's
      metadata, it reads in all of the metadata for the container and
      caches it.  This includes the segment hashes, and is thus enough
      to reconstruct a mini-index of that container.  Given the
      spatial locality of segments to a container, a huge number of
      subsequent reads and writes are likely to hit in the same
      container and thus hit this cache.  I suspect that part of the
      reason this is so effective for them is that they're consuming
      backup streams, which will tend to be in the same order as the
      previous stream.
      \begin{sidenote}
        Their locality preserving caching will suffer from locality
        degradation over time because unchanged segments will become
        increasingly scattered across old containers.  This is visible
        in their read throughput graph and is mentioned very briefly
        at the end of section 5.4, but is not otherwise addressed.
      \end{sidenote}
    \end{description}
    
    \par Their space savings analysis is largely useless because it
    compares the total space consumed by a set of \emph{full} backups
    to the deduplicated space.  The backed up file system could be
    completely unchanging and their ``compression ratio'' would increase
    linearly with time.
  }
}

@inproceedings{bolosky00sis,
  author = {William J. Bolosky and Scott Corbin and David Goebel
                  and John R. Douceur},
  title = {Single instance storage in {W}indows\textregistered
                  2000},
  crossref = {wss00},
xpages = {13--24},
  url = {http://www.usenix.org/events/usenix-win2000/bolosky.html},
  keywords = {dedup},
  abstract = {Certain applications, such as Windows 2000's Remote
                  Install service, can result in a set of files in
                  which many different files have the same
                  content. Using a traditional file system to store
                  these files separately results in excessive use of
                  disk and main memory file cache space. Using hard or
                  symbolic links would eliminate the excess resource
                  requirements, but changes the semantics of having
                  separate files, in that updates to one ``copy'' of a
                  file would be visible to users of another ``copy.''
                  We describe the Single Instance Store (SIS), a
                  component within WindowsÂ® 2000 that implements links
                  with the semantics of copies for files stored on a
                  Windows 2000 NTFS volume. SIS uses copy-on-close to
                  implements the copy semantics of its links. SIS is
                  structured as a file system filter driver that
                  implements links and a user level service that
                  detects duplicate files and reports them to the
                  filter for conversion into links. Because SIS links
                  are semantically identical to separate files, SIS
                  creates them automatically when it detects files
                  with duplicate contents. This paper describes the
                  design and implementation of SIS in detail, briefly
                  presents measurements of a remote install server
                  showing a 58\% disk space savings by using SIS, and
                  discusses other possible uses of SIS.},
  annote = {
    Operates on whole files
    
    \par Two layer design: A kernel-level file system filter
    implements ``links'' with COW-ish semantics (actually
    copy-on-close, though I never figured out what that meant) and a
    user-level groveller that finds duplicate files and uses the
    filter to create links.  Takes advantage of NTFS's ``update
    journal'' to feed the groveller and NTFS ``reparse points'' to
    make special files that are handled by the filter.
    
    \par Deduplicated files go into a common store and all equivalent
    files become links to these files.  Non-deduplicated files are
    unaffected.  Writes to a deduplicated file will that copy back
    into the file system and add it to the groveller work queue.
    
    \par The groveler signature is comprised of the file size and the
    hash of two 4K blocks of the file, on the premise that they have
    to deal with hash collisions anyways and this reduces the I/O.
    
    \par Interesting measurements of a ``remote install'' server
    storing 20 different versions of Windows.  Reduced 45,000 files
    (39\%) in 6.0 gigs (80\%) to 13,000 common files requiring 1.6
    gigs.
  }
}

@inproceedings{barreto05contentbased,
  author = {Jo{\~{a}}o Barreto and Paulo Ferreira},
  title = {Efficient File Storage Using Content-Based Indexing},
  crossref = {sosp05},
xpages = {1--9},
  url = {http://doi.acm.org/10.1145/1095810.1118597},
  keywords = {dedup},
  abstract = {Content-based indexing [MCM01] is a technique of
                  proven effectiveness for efficient transference of
                  file contents over low bandwidth network
                  links. Departing from this context, the natural step
                  of extending the application of this technique to
                  local file storage has been proposed by a number of
                  storage solutions [CN02, QD02, BF04]. To some
                  extent, all these solutions share a core storage
                  model. File contents are divided into disjoint
                  chunks of data, each of which is individually
                  stored, along with a unique hash of its contents, in
                  a repository of chunks. The actual files are then
                  stored as sequences of possibly shared references to
                  chunks in the repository.},
  annote = {
    Leaves non-duplicate blocks in regular files instead of using a
    repository for all blocks.  Note that they also store
    \emph{duplicate} blocks in regular files, simply leaving them in
    the file that contained them first.  No details are given on what
    happens when such blocks are written to.
  }
}

@inproceedings{quinlan02venti,
  author = {Sean Quinlan and Sean Dorward},
  title = {{V}enti: A New Approach to Archival Data Storage},
  crossref = {fast02},
xpages = {89--102},
  url = {http://www.usenix.org/publications/library/proceedings/fast02/quinlan.html},
  keywords = {dedup},
  abstract = {This paper describes a network storage system,
                  called Venti, intended for archival data. In this
                  system, a unique hash of a block's contents acts as
                  the block identifier for read and write
                  operations. This approach enforces a write-once
                  policy, preventing accidental or malicious
                  destruction of data. In addition, duplicate copies
                  of a block can be coalesced, reducing the
                  consumption of storage and simplifying the
                  implementation of clients. Venti is a building block
                  for constructing a variety of storage applications
                  such as logical backup, physical backup, and
                  snapshot file systems.  \par We have built a
                  prototype of the system and present some preliminary
                  performance results. The system uses magnetic disks
                  as the storage technology, resulting in an access
                  time for archival data that is comparable to
                  non-archival data. The feasibility of the write-once
                  model for storage is demonstrated using data from
                  over a decade's use of two Plan 9 file systems.},
  annote = {
    Archival storage, so once a block is in Venti, it's always in
    Venti.  This simplifies a lot of their design.
    
    \par An index maps fingerprints to block locations.  All blocks
    are stored in an append-only log broken into fixed-size arenas.
    The index is stored as a simple on-disk linear probed hash table.
    Large caches are used for both the index and the blocks in order
    to reduce the impact of random reads.
    
    \par The current version of Venti depends on Bloom filters for a
    lot of its performance, though I'm not sure exactly how they're
    used.
  }
}





@proceedings{fast02,
  title = the # fast02,
  booktitle = procofthe # fast02,
  month = jan,
  year = 2002,
  address = amonterey,
  organization = ousenix
}


@proceedings{fast08,
  title = the # fast08,
  booktitle = procofthe # fast08,
  month = feb,
  year = 2008,
  address = asanjose,
  organization = ousenix
}


@proceedings{fast09,
  title = the # fast09,
  booktitle = procofthe # fast09,
  month = feb,
  year = 2009,
  address = asanfrancisco,
  organization = ousenix
}



@proceedings{icdcs02,
  title = the # icdcs02,
  booktitle = procofthe # icdcs02,
  year = 2002,
  address = avienna,
  month = jul,
  organization = oieee
}



@proceedings{ifl98,
  title = the # ifl98,
  booktitle = procofthe # ifl98,
  month = sep,
  year = 1998,
  series = slncs,
  volume = 1595,
  address = alondon,
  publisher = pspringer,
  isbn = {3-540-66229-4},
  key = {IFL '98}
}



@proceedings{mss99,
  title = the # mss99,
  booktitle = procofthe # mss99,
  month = mar,
  year = 1999,
  address = asandiego,
  organization = oieee
}


@proceedings{mss04,
  title = the # mss04,
  booktitle = procofthe # mss04,
  month = apr,
  year = 2004,
  address = agoddard,
  organization = oieee
}



@proceedings{osdi02,
  title = the # osdi02,
  booktitle = procofthe # osdi02,
  month = dec,
  year = 2002,
  address = aboston,
  organization = ousenix
}


@proceedings{osdi06,
  title = the # osdi06,
  booktitle = procofthe # osdi06,
  month = nov,
  year = 2006,
  address = aseattle,
  organization = ousenix
}


@proceedings{osdi08,
  title = the # osdi08,
  booktitle = procofthe # osdi08,
  month = dec,
  year = 2008,
  address = asandiego,
  organization = ousenix
}



@proceedings{sosp01,
  title = the # sosp01,
  booktitle = procofthe # sosp01,
  month = oct,
  year = 2001,
  address = alakelouise,
  isbn = {1-58113-389-8},
  organization = oacm
}


@proceedings{sosp03,
  title = the # sosp03,
  booktitle = procofthe # sosp03,
  month = oct,
  year = 2003,
  address = aboltonlanding,
  isbn = {1-58113-757-5},
  organization = oacm
}


@proceedings{sosp05,
  title = the # sosp05,
  booktitle = procofthe # sosp05,
  month = oct,
  year = 2005,
  address = abrighton,
  isbn = {1-59593-079-5},
  organization = oacm
}


@proceedings{sosp07,
  title = the # sosp07,
  booktitle = procofthe # sosp07,
  month = oct,
  year = 2007,
  address = astevenson,
  isbn = {978-1-59593-591-5},
  organization = oacm
}



@proceedings{wss00,
  title = the # wss00,
  booktitle = procofthe # wss00,
  month = aug,
  year = 2000,
  address = aseattle,
  organization = ousenix
}

